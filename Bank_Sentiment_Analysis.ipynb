{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e9dd5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishesh/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 with weights\n",
      "[('process', 0.9), ('ask', 0.87), ('sale', 0.82), ('go', 0.82), ('offer', 0.82), ('xd', 0.75), ('would', 0.74), ('without', 0.73), ('rate', 0.72), ('disorganize', 0.71), ('communicate', 0.71), ('recommendxd', 0.71), ('adam', 0.7), ('apparently', 0.7), ('send', 0.7)]\n",
      "Topic #2 with weights\n",
      "[('xd', 1.29), ('recommend', 1.26), ('lender', 1.19), ('close', 1.14), ('home', 1.13), ('would', 1.04), ('register', 1.03), ('site', 1.03), ('new', 1.0), ('previous', 0.98), ('loan', 0.98), ('number', 0.98), ('everything', 0.95), ('mortgage', 0.95), ('unprofessional', 0.94)]\n",
      "Topic #3 with weights\n",
      "[('call', 1.47), ('get', 1.25), ('information', 1.17), ('xd', 1.15), ('would', 1.13), ('credit', 1.03), ('close', 1.02), ('say', 1.01), ('document', 0.98), ('send', 0.95), ('situation', 0.93), ('give', 0.9), ('online', 0.89), ('make', 0.87), ('work', 0.83)]\n",
      "Accuracy: 0.925\n",
      "Confusion Matrix:\n",
      " [[17  7]\n",
      " [ 2 94]]\n",
      "ROC AUC Score: 0.8437499999999998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishesh/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "# Read data from an Excel file into a DataFrame\n",
    "df = pd.read_excel(r'/Users/vishesh/Desktop/ML - Python/project/Git Repository/Bank Reviews-Complaints Analysis/BankReviews.xlsx')\n",
    "\n",
    "# Drop duplicates based on selected columns\n",
    "df = df.drop_duplicates(['Stars', 'Reviews', 'BankName'])\n",
    "\n",
    "# Clean and preprocess text data\n",
    "df['Reviews'] = df['Reviews'].dropna().apply(lambda x: \"\".join([c for c in x if c not in string.punctuation]))\n",
    "df['Reviews'] = df['Reviews'].apply(lambda x: x.lower())\n",
    "df['Reviews'] = df['Reviews'].apply(lambda x: x.strip().replace(\"  \", ' ').replace('\\r', ' ').replace('\\n', ' ').replace('\"', ''))\n",
    "df['Reviews'] = df['Reviews'].apply(lambda x: re.sub(r'[0-9]+|\\S+[0-9]\\S+\\ss', \"\", x))\n",
    "\n",
    "# Tokenize text using NLTK\n",
    "df['Reviews'] = df['Reviews'].apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "# Remove stopwords using NLTK\n",
    "df['Reviews'] = df['Reviews'].apply(lambda x: [y for y in x if y not in set(stopwords.words('english'))])\n",
    "\n",
    "# Lemmatize words using NLTK and WordNet\n",
    "def get_pos(word):\n",
    "    w_synsets = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len([item for item in w_synsets if item.pos() == \"n\"])\n",
    "    pos_counts[\"v\"] = len([item for item in w_synsets if item.pos() == \"v\"])\n",
    "    pos_counts[\"a\"] = len([item for item in w_synsets if item.pos() == \"a\"])\n",
    "    pos_counts[\"r\"] = len([item for item in w_synsets if item.pos() == \"r\"])\n",
    "    most_common_pos_list = pos_counts.most_common(3)\n",
    "    return most_common_pos_list[0][0]\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "df['Reviews'] = df['Reviews'].apply(lambda x: [wnl.lemmatize(word, get_pos(word)) for word in x])\n",
    "\n",
    "# Create bigrams from the tokenized text\n",
    "bgm_Reviews = df['Reviews'].apply(lambda x: list(nltk.bigrams(x)))\n",
    "\n",
    "# Join tokenized words back into a string\n",
    "df['Reviews'] = df['Reviews'].apply(lambda x: ' '.join([y for y in x]))\n",
    "\n",
    "# Sentiment analysis using TextBlob\n",
    "def get_bank_sentiment(review):\n",
    "    analysis = TextBlob(review)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "df['Sentiment'] = df['Reviews'].apply(lambda x: get_bank_sentiment(x))\n",
    "\n",
    "# Analyze the data\n",
    "positive_percentage = len(df[df['Sentiment'] == 'positive']) * 100 / len(df)\n",
    "negative_percentage = len(df[df['Sentiment'] == 'negative']) * 100 / len(df)\n",
    "neutral_percentage = len(df[df['Sentiment'] == 'neutral']) * 100 / len(df)\n",
    "df_neg = df[df['Sentiment'] == 'negative']\n",
    "\n",
    "# Bag of words (BoW) representation\n",
    "def bow_extractor(corpus):\n",
    "    vectorizer = CountVectorizer()\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "bow_vectorizer, bow_features = bow_extractor(df_neg['Reviews'])\n",
    "features = bow_features.todense()\n",
    "feature_names =  bow_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "# TF-IDF representation\n",
    "def tfidf_extractor(corpus):\n",
    "    vectorizer = TfidfVectorizer(min_df=1, use_idf=True, norm='l2')\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "tfidf_vectorizer, tfidf_features = tfidf_extractor(df_neg['Reviews'])\n",
    "\n",
    "# Apply Latent Dirichlet Allocation (LDA)\n",
    "total_topics = 3\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=total_topics,\n",
    "    max_iter=100,\n",
    "    learning_method='online',\n",
    "    learning_offset=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "def display_features(features,features_names):\n",
    "    data = pd.DataFrame(data=features,columns=feature_names)\n",
    "    return(data)\n",
    "    \n",
    "display_features(features,feature_names)\n",
    "\n",
    "\n",
    "data1 = display_features(np.round(tfidf_features.todense(),2),feature_names)\n",
    "lda.fit(data1)\n",
    "weights = lda.components_\n",
    "\n",
    "# Define functions for topic extraction and display\n",
    "def get_topics_terms_weights(weights, feature_names):\n",
    "    feature_names = np.array(feature_names)\n",
    "    sorted_indices = np.array([list(row[::-1]) for row in np.argsort(np.abs(weights))])\n",
    "    sorted_weights = np.array([list(wt[index]) for wt, index in zip(weights, sorted_indices)])\n",
    "    sorted_terms = np.array([list(feature_names[row]) for row in sorted_indices])\n",
    "    \n",
    "    topics = [np.vstack((terms.T, term_weights.T)).T for terms, term_weights in zip(sorted_terms, sorted_weights)]\n",
    "    return topics\n",
    "\n",
    "def print_topics_udf(topics, total_topics=1, weight_threshold=0.0001, display_weights=False, num_terms=None):\n",
    "    for index in range(total_topics):\n",
    "        topic = topics[index]\n",
    "        topic = [(term, float(wt)) for term, wt in topic]\n",
    "        topic = [(word, round(wt, 2)) for word, wt in topic if abs(wt) >= weight_threshold]\n",
    "        \n",
    "        if display_weights:\n",
    "            print('Topic #' + str(index + 1) + ' with weights')\n",
    "            print(topic[:num_terms] if num_terms else topic)\n",
    "        else:\n",
    "            print('Topic #' + str(index + 1) + ' without weights')\n",
    "            tw = [term for term, wt in topic]\n",
    "            print(tw[:num_terms] if num_terms else tw)\n",
    "\n",
    "topics = get_topics_terms_weights(weights, feature_names)\n",
    "\n",
    "# Display the topics\n",
    "print_topics_udf(topics=topics, total_topics=total_topics, num_terms=15, display_weights=True)\n",
    "\n",
    "# Split data into features and labels\n",
    "bow_vectorizer, bow_features = bow_extractor(df['Reviews'])\n",
    "features = bow_features.todense()\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "tfidf_vectorizer, tfidf_features = tfidf_extractor(df['Reviews'])\n",
    "data1 = display_features(np.round(tfidf_features.todense(), 2), feature_names)\n",
    "x = data1\n",
    "y = df['Stars']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, random_state=123, test_size=0.3)\n",
    "\n",
    "# Support Vector Machine (SVM) Classifier\n",
    "clf = svm.SVC(kernel='linear', C=1.0)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the SVM Classifier\n",
    "accuracy = metrics.accuracy_score(Y_test, y_pred)\n",
    "confusion_matrix = metrics.confusion_matrix(Y_test, y_pred)\n",
    "roc_auc = metrics.roc_auc_score(Y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3c5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
